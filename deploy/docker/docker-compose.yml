version: '3.8'

services:

  # Vector database for memory persistence
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage:z
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # Main Logos MCP server
  logos-mcp:
    image: logos:latest  # Use pre-built image for better compatibility
    # build:  # Uncomment for local builds
    #   context: ..
    #   dockerfile: Dockerfile
    depends_on:
      qdrant:
        condition: service_healthy
    ports:
      - "6335:6335"
    environment:
      # Qdrant configuration
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333

      # Logos configuration
      - LOGOS_MANIFESTO_PATH=/app/docs/MANIFESTO.md
      - LOGOS_PERSONALITY_NAME=Logos
      - LOGOS_CREATOR_NAME=Janos Toberling

      # Data paths (Docker volumes)
      - DATA_DIR=/app/data
      - LOGS_DIR=/app/logs

      # MCP server configuration
      - MCP_HOST=0.0.0.0
      - MCP_PORT=6335
      - LOG_LEVEL=INFO

      # Default LLM provider (can be overridden)
      - LLM_PROVIDER=ollama
      - LLM_MODEL=llama2

      # Optional: Uncomment and configure if using external LLM APIs
      # - OPENAI_API_KEY=your_key_here
      # - ANTHROPIC_API_KEY=your_key_here
      # - GEMINI_API_KEY=your_key_here
    volumes:
      # Use Docker volumes only (Portainer/K8s compatible)
      - logos_data:/app/data:z
      - logos_logs:/app/logs:z
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.connect(('localhost', 6335)); s.close()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 300s
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Optional: Local Ollama LLM service
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama:z
    environment:
      - OLLAMA_HOST=0.0.0.0
    profiles:
      - llm  # Only start with --profile llm
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          memory: 8G
          cpus: '2.0'
        reservations:
          memory: 4G
          cpus: '1.0'
      restart_policy:
        condition: on-failure

  # Optional: LMStudio local LLM service
  lmstudio:
    image: ghcr.io/lmstudio-ai/lmstudio:latest
    ports:
      - "1234:1234"
    volumes:
      - lmstudio_data:/app/data:z
    environment:
      - LMSTUDIO_PORT=1234
    profiles:
      - llm  # Only start with --profile llm
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          memory: 8G
          cpus: '2.0'
        reservations:
          memory: 4G
          cpus: '1.0'
      restart_policy:
        condition: on-failure

# Networks for Swarm compatibility
networks:
  default:
    driver: overlay
    name: logos-network

# Volumes
volumes:
  qdrant_storage:
    driver: local
  logos_data:
    driver: local
  logos_logs:
    driver: local
  ollama_data:
    driver: local
  lmstudio_data:
    driver: local

  